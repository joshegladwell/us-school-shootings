<!DOCTYPE html>
<html>
    
    <head>
        <title>Decision Trees</title>

        <!-- point to css stylesheet -->
        <link rel="stylesheet" href="../styles.css">
    </head>


    <body>
        <div class="header">
            <div class="header-center">
                <a class="logo" href="../index.html">jg</a>
            </div>
            <div class="header-sub">
                Joshua Gladwell
            </div>
            <div class="header-tabs">
                <a href="./about_me.html">About Me</a>
                <a href="https://github.com/anly501/anly-501-project-joshegladwell" target="_blank">Code</a>
                <a href="https://github.com/anly501/anly-501-project-joshegladwell/tree/main/data" target="_blank">Data</a>
                <a href="./introduction.html">Introduction</a>
                <a href="./data_gathering.html">Data Gathering</a>
                <a href="./data_cleaning.html">Data Cleaning</a>
                <a href="./exploring_data.html">Exploring Data</a>
                <a href="./naive_bayes.html">Na&iuml;ve Bayes</a>
                <a href="./decision_trees.html">Decision Trees</a>
                <a href="./svm.html">SVM</a>
                <a href="./clustering.html">Clustering</a>
                <a href="./arm_networking.html">ARM and Networking</a>
                <a href="./conclusions.html">Conclusions</a>
            </div>
        </div>
        <div class="text">
            <h1>Decision Trees</h1>

            <h2>Methods</h2>
            <p>Decision trees are a common machine learning technique for both classification and regression problems. In classification problems, we use data to make a prediction of discrete possibilities (for example, whether it will be rainy, snowy, sunny, or cloudy on a given day). On the other hand, in regression problems we use data to make a numerical prediction (for example, the high temperature for a given day). In this scenario, (as in the Na&iuml;ve Bayes section of this project), we are dealing with a classification problem.</p>
            <p>Decision trees identify patterns in the data to construct a hierarchical tree with a single "root node" at the top, and layers of nodes branching down into leaves. The leaf nodes are the end points in the decision tree and determine the classification of the given data.</p>
            <p>For example, consider a decision tree designed to classify weather data. Suppose we have features such as the previous day's temparature, air pressure, humidity, or other features that may contribute to an accurate prediction of the next day's forecast. The root node may have a "decision" such as whether or not the temperature from the given input was greater than 70 degrees Fahrenheit. If the condition is met, we traverse left down the tree to the next node, or we traverse right if it is not met. This process continues until we reach a leaf node that finally declares the predicted label (sunny, rainy, etc.) for the given input.</p>

            <h2>Class Distribution</h2>
            <p>In this section, we take the same task as in the Na&iuml;ve Bayes section of predicting whether a given tweet about a school shooting is expressing an opinion about the incident or conveying news about the incident. After we import our data, we check the balance of labels in our dataset:</p>

            <table class="center-table">
                <tr>
                    <th></th>
                    <th>Count</th>
                    <th>Percentage</th>
                </tr>
                <tr>
                    <th>Opinion</th>
                    <td>16,944</td>
                    <td>53.7%</td>
                </tr>
                <tr>
                    <th>News</th>
                    <td>14,580</td>
                    <td>46.3%</td>
                </tr>
            </table>

            <p>As we can see, the data is very well-balanced, meaning the number of "Opinion" tweets is roughly equal to the number of "News" tweets. This is desirable for the algorithm results.</p>

            <h2>Feature Selection</h2>
            <p>Because we performed this analysis in the Naive Bayes section, there is no need for further feature selection or data preprocessing.</p>

            <h2>Model Tuning</h2>
            <p>We will now determine the optimal depth for the decision tree by training 20 decision trees at varying maximum depths.</p>
            <img src="../images/dt/accuracy.png" width="1200">
            <img src="../images/dt/recallY0.png" width="1200">
            <img src="../images/dt/recallY1.png" width="1200">

            <p>As we can see in the plots above, the test set generally does not improve with deeper decision trees. There is a slight improvement in recall of the opinion class at a depth of 5, but that is the only indication of improvement. Therefore, we will train the model at a maximum depth of 5.</p>

            <h2>Final Results</h2>
            <p>Using the maximum depth of 5, we train our final model.</p>
            <img src="../images/dt/cm.png" width="1200">

            <p>Clearly there is a level of overfit in the decision tree model, even at a low maximum depth. Considering the hyperparameter tuning plots, it seems this is purely an issue with using decision trees for this specific problem. They do not seem to be well-optimized for the nature of this problem.</p>

            <h2>Conclusions</h2>
            <p>All in all, this analysis went poorly. No matter how much tuning we performed on the decision tree model, it was never able to perform accurate predictions of the nature of a given tweet. It tended to prefer predicting a tweet as an opinion tweet, even when it was not.</p>
            <p>In the future, I would stick to Na&iuml;ve Bayes algorithms for this kind of analysis.</p>
        </div>
    </body>
</html>
